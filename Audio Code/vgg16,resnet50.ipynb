{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 images belonging to 2 classes.\n",
      "Found 36 images belonging to 2 classes.\n",
      "Epoch 1/70\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.7990 - accuracy: 0.4375 - val_loss: 0.8254 - val_accuracy: 0.5625\n",
      "Epoch 2/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4002 - accuracy: 0.8000 - val_loss: 2.4994 - val_accuracy: 0.4375\n",
      "Epoch 3/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5746 - accuracy: 0.6000 - val_loss: 8.2621 - val_accuracy: 0.4062\n",
      "Epoch 4/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 7.6466 - accuracy: 0.6000 - val_loss: 7.3617 - val_accuracy: 0.4375\n",
      "Epoch 5/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 6.4794 - accuracy: 0.4000 - val_loss: 4.8615 - val_accuracy: 0.4062\n",
      "Epoch 6/70\n",
      "1/1 [==============================] - 6s 6s/step - loss: 5.0489 - accuracy: 0.4062 - val_loss: 1.3467 - val_accuracy: 0.4375\n",
      "Epoch 7/70\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.2578 - accuracy: 0.4375 - val_loss: 3.2097 - val_accuracy: 0.5312\n",
      "Epoch 8/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.2824 - accuracy: 0.6000 - val_loss: 3.7500 - val_accuracy: 0.5938\n",
      "Epoch 9/70\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4.2811 - accuracy: 0.5625 - val_loss: 3.9935 - val_accuracy: 0.5938\n",
      "Epoch 10/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.4644 - accuracy: 0.5625 - val_loss: 3.0303 - val_accuracy: 0.5625\n",
      "Epoch 11/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 5.7072 - accuracy: 0.4000 - val_loss: 2.5908 - val_accuracy: 0.5625\n",
      "Epoch 12/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 3.6033 - accuracy: 0.4000 - val_loss: 1.4431 - val_accuracy: 0.5312\n",
      "Epoch 13/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.9330 - accuracy: 0.5000 - val_loss: 1.3429 - val_accuracy: 0.4375\n",
      "Epoch 14/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.2833 - accuracy: 0.4000 - val_loss: 2.0013 - val_accuracy: 0.5625\n",
      "Epoch 15/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.3230 - accuracy: 0.4062 - val_loss: 2.7207 - val_accuracy: 0.3750\n",
      "Epoch 16/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4056 - accuracy: 0.8000 - val_loss: 3.6082 - val_accuracy: 0.3750\n",
      "Epoch 17/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.6144 - accuracy: 0.4000 - val_loss: 2.6295 - val_accuracy: 0.3750\n",
      "Epoch 18/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.1418 - accuracy: 0.5625 - val_loss: 1.0617 - val_accuracy: 0.4375\n",
      "Epoch 19/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.3065 - accuracy: 0.3438 - val_loss: 0.8065 - val_accuracy: 0.6562\n",
      "Epoch 20/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.2870 - accuracy: 0.4062 - val_loss: 1.5581 - val_accuracy: 0.5938\n",
      "Epoch 21/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.3151 - accuracy: 0.4375 - val_loss: 2.4707 - val_accuracy: 0.5312\n",
      "Epoch 22/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.9397 - accuracy: 0.6875 - val_loss: 2.2494 - val_accuracy: 0.5938\n",
      "Epoch 23/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.0572 - accuracy: 0.5000 - val_loss: 1.3549 - val_accuracy: 0.5312\n",
      "Epoch 24/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.1675 - accuracy: 0.5000 - val_loss: 1.4027 - val_accuracy: 0.4375\n",
      "Epoch 25/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.7806 - accuracy: 0.4000 - val_loss: 1.2696 - val_accuracy: 0.4375\n",
      "Epoch 26/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 3.2707 - accuracy: 0.4000 - val_loss: 1.6606 - val_accuracy: 0.4375\n",
      "Epoch 27/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.3429 - accuracy: 0.3438 - val_loss: 1.9622 - val_accuracy: 0.5000\n",
      "Epoch 28/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.6490 - accuracy: 0.5000 - val_loss: 2.2722 - val_accuracy: 0.4062\n",
      "Epoch 29/70\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.6982 - accuracy: 0.0000e+00 - val_loss: 2.0852 - val_accuracy: 0.3750\n",
      "Epoch 30/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.9423 - accuracy: 0.5625 - val_loss: 1.5919 - val_accuracy: 0.3125\n",
      "Epoch 31/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.2231 - accuracy: 0.6000 - val_loss: 0.9471 - val_accuracy: 0.5000\n",
      "Epoch 32/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.2352 - accuracy: 0.8000 - val_loss: 1.1563 - val_accuracy: 0.5000\n",
      "Epoch 33/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.1068 - accuracy: 0.5312 - val_loss: 1.0950 - val_accuracy: 0.5938\n",
      "Epoch 34/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6459 - accuracy: 0.6000 - val_loss: 1.9086 - val_accuracy: 0.5312\n",
      "Epoch 35/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.3283 - accuracy: 0.6250 - val_loss: 2.1837 - val_accuracy: 0.5625\n",
      "Epoch 36/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.2440 - accuracy: 0.5312 - val_loss: 2.0984 - val_accuracy: 0.5938\n",
      "Epoch 37/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.4462 - accuracy: 0.5938 - val_loss: 2.4721 - val_accuracy: 0.5312\n",
      "Epoch 38/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 4.2854 - accuracy: 0.0000e+00 - val_loss: 1.9022 - val_accuracy: 0.5625\n",
      "Epoch 39/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.0474 - accuracy: 0.6000 - val_loss: 1.3598 - val_accuracy: 0.5625\n",
      "Epoch 40/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.9458 - accuracy: 0.5625 - val_loss: 1.0251 - val_accuracy: 0.6250\n",
      "Epoch 41/70\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.5644 - accuracy: 0.4000 - val_loss: 0.6677 - val_accuracy: 0.5312\n",
      "Epoch 42/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.1951 - accuracy: 0.4688 - val_loss: 0.7395 - val_accuracy: 0.4062\n",
      "Epoch 43/70\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.7659 - accuracy: 0.8000 - val_loss: 0.8713 - val_accuracy: 0.4062\n",
      "Epoch 44/70\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.9382 - accuracy: 0.6000 - val_loss: 1.0049 - val_accuracy: 0.4062\n",
      "Epoch 45/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.6491 - accuracy: 0.5312 - val_loss: 1.0420 - val_accuracy: 0.4688\n",
      "Epoch 46/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.1530 - accuracy: 0.5312 - val_loss: 0.8697 - val_accuracy: 0.4688\n",
      "Epoch 47/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.3173 - accuracy: 0.5312 - val_loss: 0.8463 - val_accuracy: 0.3750\n",
      "Epoch 48/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5413 - accuracy: 0.6000 - val_loss: 0.7459 - val_accuracy: 0.5312\n",
      "Epoch 49/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8255 - accuracy: 0.4000 - val_loss: 0.7626 - val_accuracy: 0.4375\n",
      "Epoch 50/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6013 - accuracy: 0.8000 - val_loss: 0.8479 - val_accuracy: 0.4062\n",
      "Epoch 51/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6025 - accuracy: 0.8000 - val_loss: 0.7639 - val_accuracy: 0.5625\n",
      "Epoch 52/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.2236 - accuracy: 0.5000 - val_loss: 0.7843 - val_accuracy: 0.5625\n",
      "Epoch 53/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.2594 - accuracy: 0.5000 - val_loss: 0.8462 - val_accuracy: 0.6250\n",
      "Epoch 54/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5785 - accuracy: 0.4000 - val_loss: 0.6725 - val_accuracy: 0.5938\n",
      "Epoch 55/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.1993 - accuracy: 0.5938 - val_loss: 0.7471 - val_accuracy: 0.5938\n",
      "Epoch 56/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8508 - accuracy: 0.4000 - val_loss: 0.6072 - val_accuracy: 0.7188\n",
      "Epoch 57/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.0935 - accuracy: 0.5312 - val_loss: 0.8405 - val_accuracy: 0.4375\n",
      "Epoch 58/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.4514 - accuracy: 0.8000 - val_loss: 0.8155 - val_accuracy: 0.4688\n",
      "Epoch 59/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.1881 - accuracy: 0.4062 - val_loss: 0.9291 - val_accuracy: 0.4062\n",
      "Epoch 60/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.0689 - accuracy: 0.4688 - val_loss: 0.8027 - val_accuracy: 0.4688\n",
      "Epoch 61/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.0318 - accuracy: 0.4688 - val_loss: 0.8161 - val_accuracy: 0.5000\n",
      "Epoch 62/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9091 - accuracy: 0.5000 - val_loss: 0.8046 - val_accuracy: 0.4062\n",
      "Epoch 63/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7409 - accuracy: 0.8000 - val_loss: 0.8122 - val_accuracy: 0.3750\n",
      "Epoch 64/70\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.9936 - accuracy: 0.5938 - val_loss: 0.7263 - val_accuracy: 0.5312\n",
      "Epoch 65/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4467 - accuracy: 0.4000 - val_loss: 0.6785 - val_accuracy: 0.5000\n",
      "Epoch 66/70\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.5565 - accuracy: 0.6000 - val_loss: 0.7476 - val_accuracy: 0.6250\n",
      "Epoch 67/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.9644 - accuracy: 0.4688 - val_loss: 0.7576 - val_accuracy: 0.5312\n",
      "Epoch 68/70\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.5348 - accuracy: 0.6000 - val_loss: 0.7025 - val_accuracy: 0.5625\n",
      "Epoch 69/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.6659 - accuracy: 0.6250 - val_loss: 0.7397 - val_accuracy: 0.5938\n",
      "Epoch 70/70\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.9517 - accuracy: 0.5312 - val_loss: 0.7446 - val_accuracy: 0.5625\n",
      "2/2 [==============================] - 4s 530ms/step\n",
      "[[21  0]\n",
      " [15  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74        21\n",
      "           1       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.58        36\n",
      "   macro avg       0.29      0.50      0.37        36\n",
      "weighted avg       0.34      0.58      0.43        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define data paths\n",
    "train_data_path = r\"C:\\Users\\Lenovo\\Downloads\\PD AUDIO DATA\\Spectrogram\\Read text\"\n",
    "test_data_path = r\"C:\\Users\\Lenovo\\Downloads\\PD AUDIO DATA\\Spectrogram\\Spontaneous dialogue\"\n",
    "\n",
    "# Image dimensions\n",
    "height, width = 224, 224\n",
    "channels = 3  # For RGB images\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(height, width),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # 'binary' for binary classification\n",
    "    color_mode='rgb',  # Change to 'grayscale' if your images are grayscale\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    target_size=(height, width),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "# Build the model (VGG16 with custom classifier)\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(height, width, channels))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 70\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // test_generator.batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_test_true = test_generator.classes\n",
    "y_test_pred = model.predict(test_generator)\n",
    "y_test_pred_class = np.round(y_test_pred)\n",
    "\n",
    "print(confusion_matrix(y_test_true, y_test_pred_class))\n",
    "print(classification_report(y_test_true, y_test_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.3108 - accuracy: 1.0000 - val_loss: 1.3903 - val_accuracy: 0.6250\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.5272 - accuracy: 0.5625 - val_loss: 1.3569 - val_accuracy: 0.5625\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5624 - accuracy: 0.5000 - val_loss: 0.9623 - val_accuracy: 0.5938\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.9312 - accuracy: 0.6250 - val_loss: 0.7131 - val_accuracy: 0.5938\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4586 - accuracy: 0.2500 - val_loss: 0.6985 - val_accuracy: 0.3438\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.7528 - accuracy: 0.4688 - val_loss: 0.7935 - val_accuracy: 0.4375\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8964 - accuracy: 0.4375 - val_loss: 0.8301 - val_accuracy: 0.4688\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7295 - accuracy: 0.5000 - val_loss: 0.8375 - val_accuracy: 0.4062\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.9366 - accuracy: 0.4062 - val_loss: 0.7753 - val_accuracy: 0.4375\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.9635 - accuracy: 0.4375 - val_loss: 0.6931 - val_accuracy: 0.4062\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7889 - accuracy: 0.7500 - val_loss: 0.6683 - val_accuracy: 0.5938\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5303 - accuracy: 0.7500 - val_loss: 0.7021 - val_accuracy: 0.5938\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.7454 - accuracy: 0.5938 - val_loss: 0.7756 - val_accuracy: 0.5625\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5476 - accuracy: 0.7500 - val_loss: 0.8751 - val_accuracy: 0.5625\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7430 - accuracy: 0.5938 - val_loss: 0.9269 - val_accuracy: 0.5625\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8431 - accuracy: 0.5000 - val_loss: 0.9102 - val_accuracy: 0.5625\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8366 - accuracy: 0.5000 - val_loss: 0.9618 - val_accuracy: 0.5312\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8243 - accuracy: 0.5000 - val_loss: 0.7472 - val_accuracy: 0.5938\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8109 - accuracy: 0.5625 - val_loss: 0.7701 - val_accuracy: 0.5312\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6089 - accuracy: 0.7500 - val_loss: 0.7297 - val_accuracy: 0.5938\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7204 - accuracy: 0.6562 - val_loss: 0.7116 - val_accuracy: 0.5625\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7306 - accuracy: 0.4688 - val_loss: 0.6794 - val_accuracy: 0.5625\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8623 - accuracy: 0.5000 - val_loss: 0.6731 - val_accuracy: 0.6250\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6903 - accuracy: 0.6562 - val_loss: 0.7387 - val_accuracy: 0.4062\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.8523 - accuracy: 0.4375 - val_loss: 0.7645 - val_accuracy: 0.4062\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002054ED584A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 3s 301ms/step\n",
      "[[ 0 21]\n",
      " [ 0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.43      1.00      0.60        16\n",
      "\n",
      "    accuracy                           0.43        37\n",
      "   macro avg       0.22      0.50      0.30        37\n",
      "weighted avg       0.19      0.43      0.26        37\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define data paths\n",
    "test_data_path = r\"C:\\Users\\Lenovo\\Downloads\\PD AUDIO DATA\\Spectrogram\\Read text\"\n",
    "train_data_path = r\"C:\\Users\\Lenovo\\Downloads\\PD AUDIO DATA\\Spectrogram\\Spontaneous dialogue\"\n",
    "\n",
    "# Image dimensions\n",
    "height, width = 224, 224\n",
    "channels = 3  # For RGB images\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(height, width),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',  # 'binary' for binary classification\n",
    "    color_mode='rgb',  # Change to 'grayscale' if your images are grayscale\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    target_size=(height, width),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "# Build the model (ResNet50 with custom classifier)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(height, width, channels))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 25\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // test_generator.batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_test_true = test_generator.classes\n",
    "y_test_pred = model.predict(test_generator)\n",
    "y_test_pred_class = np.round(y_test_pred)\n",
    "\n",
    "print(confusion_matrix(y_test_true, y_test_pred_class))\n",
    "print(classification_report(y_test_true, y_test_pred_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
